{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of Linear decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook, I am going to discuss how Linear decision trees can be added to Gogeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "using EvoTrees\n",
    "using JuMP\n",
    "using Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference is that Linear decision trees have a linear function at the leafs, while normal decision tree has a coefficient. Lest create a dataset to learn a simple function:\n",
    "$$f(x) = \\sum_{i=1}^5 x_i^3$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rand(1000, 5) .- 0.5;\n",
    "x_train = data[1:750, :];\n",
    "y_train = vec(sum(map.(x->x^3, x_train), dims=2));\n",
    "\n",
    "x_test = data[751:end, :];\n",
    "y_test = vec(sum(map.(x->x^3, x_test), dims=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using `EvoTrees`, lets create forest of 2 trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EvoTreeRegressor(nrounds=1, max_depth=5);\n",
    "evo_model = fit_evotree(config; x_train, y_train, verbosity=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tree is always bias tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvoTrees.Tree{EvoTrees.MSE, 1}\n",
       " - feat: [0]\n",
       " - cond_bin: UInt8[0x00]\n",
       " - cond_float: Any[0.0]\n",
       " - gain: [0.0]\n",
       " - pred: Float32[-0.0028208059;;]\n",
       " - split: Bool[0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evo_model.trees[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are predictions learned by general tree. With Linear decision tree, we would also have $R^n$ vector for each leaf in the tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element Vector{Float32}:\n",
       " -0.013679789\n",
       " -0.004538178\n",
       " -0.009680798\n",
       " -0.0010463883\n",
       "  0.0016119897\n",
       "  0.010375644\n",
       "  0.013378836\n",
       "  0.02618761\n",
       " -0.0073071234\n",
       "  0.009128554\n",
       " -0.0013803802\n",
       "  0.012229258\n",
       "  0.003046792\n",
       "  0.011406319\n",
       "  0.01262668\n",
       "  0.02374924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaves = findall(node -> evo_model.trees[2].split[node] == false && (node == 1 || evo_model.trees[2].split[floor(Int, node / 2)] == true), 1:length(evo_model.trees[2].split))\n",
    "\n",
    "evo_model.trees[2].pred[leaves]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Linear decision trees to be introduced in Julia, I would expect them to have similar structure as `EvoTrees`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LTEModel\n",
    "    n_trees::Int64\n",
    "    n_feats::Int64\n",
    "    n_leaves::Array{Int64}\n",
    "    leaves::Array{Array}\n",
    "    splits::Matrix{Any}\n",
    "    splits_ordered::Array{Vector}\n",
    "    n_splits::Array{Int64}\n",
    "    a::Array{Array}\n",
    "    b::Array{Array}\n",
    "    split_nodes::Array{Array}\n",
    "    child_leaves::Array{Array}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extract_evotrees_info (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function extract_evotrees_info(evo_model; tree_limit=length(evo_model.trees))\n",
    "\n",
    "    n_trees = tree_limit\n",
    "    n_feats = length(evo_model.info[:fnames])\n",
    "\n",
    "    n_leaves = Array{Int64}(undef, n_trees) # number of leaves on each tree\n",
    "    leaves = Array{Array}(undef, n_trees) # ids of the leaves of each tree\n",
    "\n",
    "    # Get number of leaves and ids of the leaves on each tree\n",
    "    for tree in 1:n_trees\n",
    "        leaves[tree] = findall(node -> evo_model.trees[tree].split[node] == false && (node == 1 || evo_model.trees[tree].split[floor(Int, node / 2)] == true), 1:length(evo_model.trees[tree].split))\n",
    "        n_leaves[tree] = length(leaves[tree])\n",
    "    end\n",
    "\n",
    "    splits = Matrix{Any}(undef, n_trees, length(evo_model.trees[2].split)) # storing the feature number and splitpoint index for each split node\n",
    "    splits_ordered = Array{Vector}(undef, n_feats) # splitpoints for each feature\n",
    "\n",
    "    n_splits = zeros(Int64, n_feats)\n",
    "    [splits_ordered[feat] = [] for feat in 1:n_feats]\n",
    "\n",
    "    for tree in 1:n_trees\n",
    "        for node in eachindex(evo_model.trees[tree].split)\n",
    "            if evo_model.trees[tree].split[node] == true\n",
    "                splits[tree, node] = [evo_model.trees[tree].feat[node], evo_model.trees[tree].cond_float[node]] # save feature and split value\n",
    "                push!(splits_ordered[evo_model.trees[tree].feat[node]], evo_model.trees[tree].cond_float[node]) # push split value to splits_ordered\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    [unique!(sort!(splits_ordered[feat])) for feat in 1:n_feats] # sort splits_ordered and remove copies\n",
    "    [n_splits[feat] = length(splits_ordered[feat]) for feat in 1:n_feats] # store number of split points\n",
    "\n",
    "    for tree in 1:n_trees\n",
    "        for node in eachindex(evo_model.trees[tree].split)\n",
    "            if evo_model.trees[tree].split[node] == true\n",
    "                \n",
    "                feature::Int = splits[tree, node][1]\n",
    "                value = splits[tree, node][2]\n",
    "\n",
    "                splits[tree, node][2] = searchsortedfirst(splits_ordered[feature], value)\n",
    "\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    b = Array{Array}(undef, n_trees)\n",
    "    [b[tree] = evo_model.trees[tree].pred for tree in 1:n_trees]\n",
    "    # -------------------------------------------------------\n",
    "    # in the Linear decision tree, there should be gradient in addition to coefficient\n",
    "    a = Array{Array}(undef, n_trees)\n",
    "    [a[tree] = zeros(length(evo_model.trees[tree].pred), n_feats) for tree in 1:n_trees]\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    split_nodes = Array{Array}(undef, n_trees)\n",
    "    [split_nodes[tree] = evo_model.trees[tree].split for tree in 1:n_trees]\n",
    "\n",
    "    return LTEModel(n_trees, n_feats, n_leaves, leaves, splits, splits_ordered, n_splits, a, b, split_nodes, Array{Array}(undef, n_trees))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LTEModel(2, 5, [1, 16], Array[[1], [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]], Any[#undef #undef … #undef #undef; [1.0, 1.0] [2.0, 3.0] … #undef #undef], Vector[Any[0.18588022654089661, 0.4205677422949932], Any[0.16318949692532522, 0.17613459442551815, 0.3142543416970481], Any[-0.25592039748035383, 0.34098227422575805], Any[-0.3145710646837453, 0.1660823910604102, 0.20651100394402205, 0.3289236763233086, 0.38702371930363905], Any[-0.30684761488771506, 0.36396254886116697, 0.43382178171959634]], [2, 3, 2, 5, 3], Array[[0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]], Array[Float32[-0.0028208059;;], Float32[0.0 0.0 … 0.01262668 0.02374924]], Array[Bool[0], Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], Array[#undef, #undef])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LTE = extract_evotrees_info(evo_model; tree_limit=length(evo_model.trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes in the function `init_TEModel!()`, the only change is that we work with `LTEModel` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function children(id::Int, leaf_dict::Dict, max::Int)\n",
    "\n",
    "    result::Vector{Int} = []\n",
    "\n",
    "    function inner(num)\n",
    "        if num <= max\n",
    "            leaf_index = get(leaf_dict, num, 0)\n",
    "            if leaf_index != 0\n",
    "                push!(result, leaf_index)\n",
    "            end\n",
    "            inner(num << 1)\n",
    "            inner(num << 1 + 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    inner(id)\n",
    "\n",
    "    return result\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_LTEModel! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function init_LTEModel!(LTE::LTEModel)\n",
    "\n",
    "    leaf_dict = Array{Dict}(undef, LTE.n_trees)\n",
    "    [leaf_dict[tree] = Dict([(LTE.leaves[tree][leaf], leaf) for leaf in eachindex(LTE.leaves[tree])]) for tree in 1:LTE.n_trees]\n",
    "\n",
    "    # pre-compute all children for all active nodes of all trees\n",
    "    for tree in 1:LTE.n_trees\n",
    "        \n",
    "        nodes_with_split = findall(split -> split == true, LTE.split_nodes[tree])\n",
    "        LTE.child_leaves[tree] = Array{Any}(undef, maximum(LTE.leaves[tree]))\n",
    "\n",
    "        for node in [nodes_with_split; LTE.leaves[tree]]\n",
    "            LTE.child_leaves[tree][node] = children(node, leaf_dict[tree], last(LTE.leaves[tree]))\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_bounds = -0.5*ones(LTE.n_feats);\n",
    "U_bounds = 0.5*ones(LTE.n_feats);\n",
    "d_bounds = [-1, 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_bounds (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function calculate_bounds(model::JuMP.Model, LTE::LTEModel, leaf, tree, d_bounds)\n",
    "\n",
    "    # Something is wrong there \n",
    "    # ------------------------------------\n",
    "\n",
    "    @objective(model, Max,  LTE.a[tree][LTE.leaves[tree][leaf], :]'*model[:x]+LTE.b[tree][LTE.leaves[tree][leaf]] - model[:d][tree])\n",
    "\n",
    "    optimize!(model)\n",
    "    upper_bound = objective_value(model)\n",
    "\n",
    "    set_objective_sense(model, MIN_SENSE)\n",
    "    optimize!(model)\n",
    "\n",
    "    lower_bound = objective_value(model)\n",
    "    \n",
    "    return [upper_bound, lower_bound]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LTE_formulate! (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function LTE_formulate!(model::JuMP.Model, LTE::LTEModel, U_bounds, L_bounds; bound_tightening = \"quadratic\", d_bounds=nothing)\n",
    "\n",
    "    empty!(model)\n",
    "\n",
    " \n",
    "    if bound_tightening == \"output\" @assert !isnothing(d_bounds) \"Bounds for forest output must be provided.\" end\n",
    "\n",
    "    init_LTEModel!(LTE)\n",
    "\n",
    "    # (2.2.8)\n",
    "    @variable(model, x[1:LTE.n_feats]);\n",
    "    @constraint(model, [i = 1:LTE.n_feats], x[i] <= U_bounds[i]);\n",
    "    @constraint(model, [i = 1:LTE.n_feats], x[i] >= L_bounds[i]);\n",
    "\n",
    "    # (2.2.9)\n",
    "    @variable(model, y[i = 1:LTE.n_feats, 1:LTE.n_splits[i]], Bin);\n",
    "\n",
    "    # (2.2.10)\n",
    "    @variable(model, z[tree = 1:LTE.n_trees, 1:LTE.n_leaves[tree]], Bin); \n",
    "\n",
    "    # (2.2.5)\n",
    "    @constraint(model, [i = 1:LTE.n_feats, j = 1:(LTE.n_splits[i]-1)], y[i,j] <= y[i, j+1]); \n",
    "\n",
    "    v = []\n",
    "    for i = 1:length(LTE.splits_ordered)\n",
    "        push!(v, vcat(L_bounds[i], LTE.splits_ordered[i], U_bounds[i]));\n",
    "    end\n",
    "\n",
    "    # (2.2.6)\n",
    "    @constraint(model, [i = 1:LTE.n_feats], x[i] >=  v[i][1] + sum((v[i][j] - v[i][j-1])*(1 - y[i, j-1]) for j = 2 : LTE.n_splits[i]+1));\n",
    "\n",
    "    # (2.2.7)\n",
    "    @constraint(model, [i = 1:LTE.n_feats], x[i] <=  v[i][LTE.n_splits[i]+2] + sum((v[i][j] - v[i][j+1])* y[i, j-1] for j = 2 : LTE.n_splits[i]+1));\n",
    "\n",
    "    # (2.2.2)\n",
    "    @constraint(model, [tree = 1:LTE.n_trees], sum(z[tree, i] for i=1:LTE.n_leaves[tree]) == 1);\n",
    "    \n",
    "    for tree in 1:LTE.n_trees\n",
    "        for current_node in findall(s -> s==true, LTE.split_nodes[tree])\n",
    "\n",
    "            right_leaves = LTE.child_leaves[tree][current_node << 1 + 1];\n",
    "            left_leaves = LTE.child_leaves[tree][current_node << 1];\n",
    "\n",
    "            current_feat, current_splitpoint_index = LTE.splits[tree, current_node];\n",
    "\n",
    "            # (2.2.3)\n",
    "            @constraint(model, sum(model[:z][tree, leaf] for leaf in right_leaves) <= 1 - model[:y][current_feat, current_splitpoint_index]);\n",
    "\n",
    "            # (2.2.4)\n",
    "            @constraint(model, sum(model[:z][tree, leaf] for leaf in left_leaves) <= model[:y][current_feat, current_splitpoint_index]);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if bound_tightening==\"quadratic\"\n",
    "        @variable(model, d);\n",
    "        # (2.2.1)\n",
    "        @constraint(model, d == sum((LTE.a[tree][LTE.leaves[tree][leaf], :]'*x+LTE.b[tree][LTE.leaves[tree][leaf]])*z[tree, leaf] for tree = 1:LTE.n_trees, leaf = 1:LTE.n_leaves[tree]));\n",
    "    \n",
    "    elseif  bound_tightening==\"output\"\n",
    "\n",
    "        @variable(model, d[1:LTE.n_trees]);\n",
    "\n",
    "        # It doesn't work\n",
    "        # ---------------------\n",
    "        \n",
    "        @constraint(model, [tree = 1:LTE.n_trees], d_bounds[1]<=d[tree]<=d_bounds[2]);\n",
    "        bounds = [[calculate_bounds(model, LTE, leaf, tree, d_bounds) for leaf in 1:LTE.n_leaves[tree]] for tree in 1:LTE.n_trees];\n",
    "        # (2.2.12)\n",
    "        @constraint(model, [tree = 2:LTE.n_trees, leaf = 1:LTE.n_leaves[tree]], LTE.a[tree][LTE.leaves[tree][leaf], :]'*x+LTE.b[tree][LTE.leaves[tree][leaf]] - d[tree] <=bounds[tree][leaf][1]*(1 - z[tree, leaf]));\n",
    "        \n",
    "        # (2.2.13)\n",
    "        @constraint(model, [tree = 2:LTE.n_trees, leaf = 1:LTE.n_leaves[tree]], LTE.a[tree][LTE.leaves[tree][leaf], :]'*x+LTE.b[tree][LTE.leaves[tree][leaf]] - d[tree] >=-bounds[tree][leaf][2]*(1 - z[tree, leaf]));\n",
    "        \n",
    "        # ---------------------\n",
    "    end \n",
    "\n",
    "     \n",
    "   end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$ d_{1} $"
      ],
      "text/plain": [
       "d[1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model[:d][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-05-20\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching -(::Int64, ::Matrix{Float32})\nFor element-wise subtraction, use broadcasting with dot syntax: scalar .- array\n\nClosest candidates are:\n  -(::Real, !Matched::Complex{Bool})\n   @ Base complex.jl:321\n  -(!Matched::MutableArithmetics.Zero, ::Any)\n   @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/rewrite.jl:63\n  -(::Any, !Matched::MutableArithmetics.Zero)\n   @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/rewrite.jl:64\n  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching -(::Int64, ::Matrix{Float32})\n",
      "For element-wise subtraction, use broadcasting with dot syntax: scalar .- array\n",
      "\n",
      "Closest candidates are:\n",
      "  -(::Real, !Matched::Complex{Bool})\n",
      "   @ Base complex.jl:321\n",
      "  -(!Matched::MutableArithmetics.Zero, ::Any)\n",
      "   @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/rewrite.jl:63\n",
      "  -(::Any, !Matched::MutableArithmetics.Zero)\n",
      "   @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/rewrite.jl:64\n",
      "  ...\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] sub_mul(a::Int64, b::Matrix{Float32})\n",
      "    @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/MutableArithmetics.jl:38\n",
      "  [2] operate(::typeof(MutableArithmetics.sub_mul), ::Int64, ::Matrix{Float32})\n",
      "    @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/interface.jl:210\n",
      "  [3] operate_fallback!!(::MutableArithmetics.IsNotMutable, ::typeof(MutableArithmetics.sub_mul), ::Int64, ::Matrix{Float32})\n",
      "    @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/interface.jl:593\n",
      "  [4] operate!!(op::typeof(MutableArithmetics.sub_mul), x::Int64, args::Matrix{Float32})\n",
      "    @ MutableArithmetics ~/.julia/packages/MutableArithmetics/NIXlP/src/rewrite.jl:93\n",
      "  [5] macro expansion\n",
      "    @ ~/.julia/packages/MutableArithmetics/NIXlP/src/rewrite.jl:321 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ ~/.julia/packages/JuMP/027Gt/src/macros.jl:239 [inlined]\n",
      "  [7] macro expansion\n",
      "    @ ~/.julia/packages/JuMP/027Gt/src/macros/@constraint.jl:413 [inlined]\n",
      "  [8] (::var\"#1860#1886\"{Vector{Int64}, LTEModel, Vector{VariableRef}, Model})(tree::Int64)\n",
      "    @ Main ~/.julia/packages/JuMP/027Gt/src/Containers/macro.jl:509\n",
      "  [9] #87\n",
      "    @ ~/.julia/packages/JuMP/027Gt/src/Containers/container.jl:124 [inlined]\n",
      " [10] iterate(::Base.Generator{Base.Iterators.Enumerate{Vector{Any}}, RecipesBase.var\"#sub#4\"})\n",
      "    @ Base ./generator.jl:47 [inlined]\n",
      " [11] collect(itr::Base.Generator{JuMP.Containers.VectorizedProductIterator{Tuple{UnitRange{Int64}}}, JuMP.Containers.var\"#87#89\"{var\"#1860#1886\"{Vector{Int64}, LTEModel, Vector{VariableRef}, Model}}})\n",
      "    @ Base ./array.jl:834\n",
      " [12] map(f::Function, A::JuMP.Containers.VectorizedProductIterator{Tuple{UnitRange{Int64}}})\n",
      "    @ Base ./abstractarray.jl:3310\n",
      " [13] container(f::Function, indices::JuMP.Containers.VectorizedProductIterator{Tuple{UnitRange{Int64}}}, ::Type{JuMP.Containers.DenseAxisArray}, names::Vector{Any})\n",
      "    @ JuMP.Containers ~/.julia/packages/JuMP/027Gt/src/Containers/container.jl:123\n",
      " [14] container\n",
      "    @ JuMP.Containers ~/.julia/packages/JuMP/027Gt/src/Containers/container.jl:75 [inlined]\n",
      " [15] macro expansion\n",
      "    @ ~/.julia/packages/JuMP/027Gt/src/macros.jl:375 [inlined]\n",
      " [16] LTE_formulate!(model::Model, LTE::LTEModel, U_bounds::Vector{Float64}, L_bounds::Vector{Float64}; bound_tightening::String, d_bounds::Vector{Int64})\n",
      "    @ Main ~/Summer_trainee_position/LinearTrees-Formulation/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X26sZmlsZQ==.jl:48\n",
      " [17] top-level scope\n",
      "    @ ~/Summer_trainee_position/LinearTrees-Formulation/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X31sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "model = Model(() -> Gurobi.Optimizer());\n",
    "\n",
    "LTE_formulate!(model, LTE, U_bounds, L_bounds, bound_tightening = \"output\", d_bounds=d_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$ d_{1} + d_{2} $"
      ],
      "text/plain": [
       "d[1] + d[2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@objective(model, Min, sum(model[:d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A JuMP Model\n",
       "Minimization problem with:\n",
       "Variables: 39\n",
       "Objective function type: AffExpr\n",
       "`AffExpr`-in-`MathOptInterface.EqualTo{Float64}`: 2 constraints\n",
       "`AffExpr`-in-`MathOptInterface.GreaterThan{Float64}`: 10 constraints\n",
       "`AffExpr`-in-`MathOptInterface.LessThan{Float64}`: 50 constraints\n",
       "`QuadExpr`-in-`MathOptInterface.EqualTo{Float64}`: 1 constraint\n",
       "`VariableRef`-in-`MathOptInterface.ZeroOne`: 32 constraints\n",
       "Model mode: AUTOMATIC\n",
       "CachingOptimizer state: ATTACHED_OPTIMIZER\n",
       "Solver name: Gurobi\n",
       "Names registered in the model: d, x, y, z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 11.0.2 build v11.0.2rc0 (mac64[arm] - Darwin 23.4.0 23E224)\n",
      "\n",
      "CPU model: Apple M1 Pro\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 94 rows, 39 columns and 245 nonzeros\n",
      "Model fingerprint: 0x6446d518\n",
      "Variable types: 7 continuous, 32 integer (32 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-02, 1e+30]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [3e-01, 1e+30]\n",
      "Warning: Model contains large matrix coefficient range\n",
      "Warning: Model contains large rhs\n",
      "         Consider reformulating model or setting NumericFocus parameter\n",
      "         to avoid numerical issues.\n",
      "\n",
      "MIP start from previous solve did not produce a new incumbent solution\n",
      "\n",
      "Presolve removed 16 rows and 0 columns\n",
      "Presolve time: 0.00s\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 1 (of 8 available processors)\n",
      "\n",
      "Solution count 0\n",
      "\n",
      "Model is infeasible or unbounded\n",
      "Best objective -, best bound -, gap -\n",
      "\n",
      "User-callback calls 37, time in user-callback 0.00 sec\n"
     ]
    }
   ],
   "source": [
    "optimize!(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "MathOptInterface.ResultIndexBoundsError{MathOptInterface.ObjectiveValue}",
     "evalue": "Result index of attribute MathOptInterface.ObjectiveValue(1) out of bounds. There are currently 0 solution(s) in the model.",
     "output_type": "error",
     "traceback": [
      "Result index of attribute MathOptInterface.ObjectiveValue(1) out of bounds. There are currently 0 solution(s) in the model.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] check_result_index_bounds\n",
      "    @ ~/.julia/packages/MathOptInterface/yczX1/src/attributes.jl:207 [inlined]\n",
      "  [2] get(model::Gurobi.Optimizer, attr::MathOptInterface.ObjectiveValue)\n",
      "    @ Gurobi ~/.julia/packages/Gurobi/uP4zR/src/MOI_wrapper/MOI_wrapper.jl:3205\n",
      "  [3] get(b::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, attr::MathOptInterface.ObjectiveValue)\n",
      "    @ MathOptInterface.Bridges ~/.julia/packages/MathOptInterface/yczX1/src/Bridges/bridge_optimizer.jl:1111\n",
      "  [4] _get_model_attribute(model::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}, attr::MathOptInterface.ObjectiveValue)\n",
      "    @ MathOptInterface.Utilities ~/.julia/packages/MathOptInterface/yczX1/src/Utilities/cachingoptimizer.jl:828\n",
      "  [5] get\n",
      "    @ MathOptInterface.Utilities ~/.julia/packages/MathOptInterface/yczX1/src/Utilities/cachingoptimizer.jl:895 [inlined]\n",
      "  [6] _moi_get_result(model::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}, args::MathOptInterface.ObjectiveValue)\n",
      "    @ JuMP ~/.julia/packages/JuMP/027Gt/src/optimizer_interface.jl:663\n",
      "  [7] get(model::Model, attr::MathOptInterface.ObjectiveValue)\n",
      "    @ JuMP ~/.julia/packages/JuMP/027Gt/src/optimizer_interface.jl:683\n",
      "  [8] objective_value(model::Model; result::Int64)\n",
      "    @ JuMP ~/.julia/packages/JuMP/027Gt/src/objective.jl:54\n",
      "  [9] objective_value(model::Model)\n",
      "    @ JuMP ~/.julia/packages/JuMP/027Gt/src/objective.jl:50\n",
      " [10] top-level scope\n",
      "    @ ~/Summer_trainee_position/LinearTrees-Formulation/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X34sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "objective_value(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.016500596f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EvoTrees.predict(evo_model, value.(model[:x])')[1] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
